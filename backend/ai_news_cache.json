{"import_ai": [{"title": "Import AI 410: Eschatological AI Policy; Virology weapon test; $50m for distributed training", "summary": "Are you a bystander or an actor?", "link": "https://importai.substack.com/p/import-ai-410-eschatological-ai-policy", "published": "Mon, 28 Apr 2025 12:15:28 GMT", "source": "Import AI"}, {"title": "Import AI 409: Huawei trains a model on 8,000+ Ascend chips; 32B decentralized training run; and the era of experience and superintelligence", "summary": "Welcome to Import AI, a newsletter about AI research.", "link": "https://importai.substack.com/p/import-ai-409-huawei-trains-a-model", "published": "Mon, 21 Apr 2025 12:10:22 GMT", "source": "Import AI"}, {"title": "Import AI 408: Multi-code SWE Bench; backdoored Unitree robots; and what AI 2027 is telling us", "summary": "Calling critical points in AI safety is just as hard as timing the market - but more consequential", "link": "https://importai.substack.com/p/import-ai-408-multi-code-swe-bench", "published": "Mon, 14 Apr 2025 12:25:40 GMT", "source": "Import AI"}, {"title": "Import AI 407: DeepMind sees AGI by 2030; MouseGPT; and ByteDance's inference cluster", "summary": "There will be few bystanders in the AI revolution", "link": "https://importai.substack.com/p/import-ai-407-deepmind-sees-agi-by", "published": "Mon, 07 Apr 2025 12:50:58 GMT", "source": "Import AI"}, {"title": "Import AI 406: AI-driven software explosion; robot hands are still bad; better LLMs via pdb", "summary": "They told me humans would be kind to machines", "link": "https://importai.substack.com/p/import-ai-406-ai-driven-software", "published": "Mon, 31 Mar 2025 12:45:46 GMT", "source": "Import AI"}], "arxiv": [{"title": "TRUST: An LLM-Based Dialogue System for Trauma Understanding and\n  Structured Assessments", "summary": "Objectives: While Large Language Models (LLMs) have been widely used to\nassist clinicians and support patients, no existing work has explored dialogue\nsystems for standard diagnostic interviews and assessments. This study aims to\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\ndialogue system that replicates clinician behavior. Materials and Methods: We\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\nDialogue Acts schema specifically designed for clinical interviews.\nAdditionally, we develop a patient simulation approach based on real-life\ninterview transcripts to replace time-consuming and costly manual testing by\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\nassess the dialogue system from both the agent and patient simulation\nperspectives. Expert evaluations by conversation and clinical specialists show\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\nOur system performs at the level of average clinicians, with room for future\nenhancements in communication styles and response appropriateness. Conclusions:\nOur TRUST framework shows its potential to facilitate mental healthcare\navailability.", "link": "http://arxiv.org/abs/2504.21851v1", "published": "2025-04-30T17:58:06Z", "authors": ["Sichang Tu", "Abigail Powers", "Stephen Doogan", "Jinho D. Choi"], "source": "arXiv"}, {"title": "Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and\n  Regulation Support", "summary": "Governance institutions must respond to societal risks, including those posed\nby generative AI. This study empirically examines how public trust in\ninstitutions and AI technologies, along with perceived risks, shape preferences\nfor AI regulation. Using the nationally representative 2023 Artificial\nIntelligence, Morality, and Sentience (AIMS) survey, we assess trust in\ngovernment, AI companies, and AI technologies, as well as public support for\nregulatory measures such as slowing AI development or outright bans on advanced\nAI. Our findings reveal broad public support for AI regulation, with risk\nperception playing a significant role in shaping policy preferences.\nIndividuals with higher trust in government favor regulation, while those with\ngreater trust in AI companies and AI technologies are less inclined to support\nrestrictions. Trust in government and perceived risks significantly predict\npreferences for both soft (e.g., slowing development) and strong (e.g., banning\nAI systems) regulatory interventions. These results highlight the importance of\npublic opinion in AI governance. As AI capabilities advance, effective\nregulation will require balancing public concerns about risks with trust in\ninstitutions. This study provides a foundational empirical baseline for\npolicymakers navigating AI governance and underscores the need for further\nresearch into public trust, risk perception, and regulatory strategies in the\nevolving AI landscape.", "link": "http://arxiv.org/abs/2504.21849v1", "published": "2025-04-30T17:56:23Z", "authors": ["Justin B. Bullock", "Janet V. T. Pauketat", "Hsini Huang", "Yi-Fan Wang", "Jacy Reese Anthis"], "source": "arXiv"}, {"title": "Characterizing AI Agents for Alignment and Governance", "summary": "The creation of effective governance mechanisms for AI agents requires a\ndeeper understanding of their core properties and how these properties relate\nto questions surrounding the deployment and operation of agents in the world.\nThis paper provides a characterization of AI agents that focuses on four\ndimensions: autonomy, efficacy, goal complexity, and generality. We propose\ndifferent gradations for each dimension, and argue that each dimension raises\nunique questions about the design, operation, and governance of these systems.\nMoreover, we draw upon this framework to construct \"agentic profiles\" for\ndifferent kinds of AI agents. These profiles help to illuminate cross-cutting\ntechnical and non-technical governance challenges posed by different classes of\nAI agents, ranging from narrow task-specific assistants to highly autonomous\ngeneral-purpose systems. By mapping out key axes of variation and continuity,\nthis framework provides developers, policymakers, and members of the public\nwith the opportunity to develop governance approaches that better align with\ncollective societal goals.", "link": "http://arxiv.org/abs/2504.21848v1", "published": "2025-04-30T17:55:48Z", "authors": ["Atoosa Kasirzadeh", "Iason Gabriel"], "source": "arXiv"}, {"title": "Active Light Modulation to Counter Manipulation of Speech Visual Content", "summary": "High-profile speech videos are prime targets for falsification, owing to\ntheir accessibility and influence. This work proposes Spotlight, a low-overhead\nand unobtrusive system for protecting live speech videos from visual\nfalsification of speaker identity and lip and facial motion. Unlike predominant\nfalsification detection methods operating in the digital domain, Spotlight\ncreates dynamic physical signatures at the event site and embeds them into all\nvideo recordings via imperceptible modulated light. These physical signatures\nencode semantically-meaningful features unique to the speech event, including\nthe speaker's identity and facial motion, and are cryptographically-secured to\nprevent spoofing. The signatures can be extracted from any video downstream and\nvalidated against the portrayed speech content to check its integrity. Key\nelements of Spotlight include (1) a framework for generating extremely compact\n(i.e., 150-bit), pose-invariant speech video features, based on\nlocality-sensitive hashing; and (2) an optical modulation scheme that embeds\n>200 bps into video while remaining imperceptible both in video and live.\nPrototype experiments on extensive video datasets show Spotlight achieves AUCs\n$\\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified\nvideos. Further, Spotlight is highly robust across recording conditions, video\npost-processing techniques, and white-box adversarial attacks on its video\nfeature extraction methodologies.", "link": "http://arxiv.org/abs/2504.21846v1", "published": "2025-04-30T17:55:24Z", "authors": ["Hadleigh Schwartz", "Xiaofeng Yan", "Charles J. Carver", "Xia Zhou"], "source": "arXiv"}, {"title": "Early Exit and Multi Stage Knowledge Distillation in VLMs for Video\n  Summarization", "summary": "We introduce DEEVISum (Distilled Early Exit Vision language model for\nSummarization), a lightweight, efficient, and scalable vision language model\ndesigned for segment wise video summarization. Leveraging multi modal prompts\nthat combine textual and audio derived signals, DEEVISum incorporates Multi\nStage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance\nbetween performance and efficiency. MSKD offers a 1.33% absolute F1 improvement\nover baseline distillation (0.5%), while EE reduces inference time by\napproximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset,\nour best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing\nthe performance of significantly larger models, all while maintaining a lower\ncomputational footprint. We publicly release our code and processed dataset to\nsupport further research.", "link": "http://arxiv.org/abs/2504.21831v1", "published": "2025-04-30T17:37:55Z", "authors": ["Anas Anwarul Haq Khan", "Utkarsh Verma", "Prateek Chanda", "Ganesh Ramakrishnan"], "source": "arXiv"}], "huggingface": [], "openai_blog": [], "google_ai": []}