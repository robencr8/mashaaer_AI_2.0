import sys
from persona_controller import apply_persona, _current_persona

# Flag to track if transformers is available
TRANSFORMERS_AVAILABLE = False

try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    TRANSFORMERS_AVAILABLE = True

    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù‚ÙˆÙ„ Ø§Ù„Ù…ØªÙˆÙØ±Ø© ğŸ§ 
    MODELS = {
        "aragpt2": "aubmindlab/aragpt2-base",
        "noor": "arbml/noor-7b-v1",
        "jais": "instructlab/jais-13b",
        "falcon": "tiiuae/falcon-7b-instruct"
    }

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¹Ù‚ÙˆÙ„ ğŸ”
    _loaded_models = {}

    def load_model(name):
        if name in _loaded_models:
            return _loaded_models[name]
        print(f"ğŸ§  Loading model: {name}")
        tokenizer = AutoTokenizer.from_pretrained(MODELS[name])
        model = AutoModelForCausalLM.from_pretrained(MODELS[name])
        _loaded_models[name] = (tokenizer, model)
        return tokenizer, model

    # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ø·Ù„Ø¨
    def choose_model(context="default"):
        if context in ["ÙØ±Ø­", "Ø­Ø²Ù†", "ØºØ¶Ø¨", "Ø®ÙˆÙ"]:
            return "aragpt2"
        elif "Ù…Ø¹Ù„ÙˆÙ…Ø©" in context or "ØªØ§Ø±ÙŠØ®" in context or "ØªØ¹Ø±ÙŠÙ":
            return "noor"
        elif "Ø³Ø¤Ø§Ù„ Ø­Ø¯ÙŠØ«" in context or "ØªÙ‚Ù†ÙŠ":
            return "jais"
        elif "Ù†Ù‚Ø§Ø´ Ø·ÙˆÙŠÙ„" in context or "ØªÙØ³ÙŠØ±":
            return "falcon"
        return "aragpt2"

except ImportError as e:
    print(f"Warning: Transformers package not available: {e}")
    print(f"Local model functionality will be disabled.")
    print(f"Python version: {sys.version}")

    # Define empty functions for compatibility
    def load_model(name):
        return None, None

    def choose_model(context="default"):
        return "unavailable"

# Ø§Ù„Ø±Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø­Ù„ÙŠ ğŸš€
def generate_local_response(prompt, context="default", max_tokens=150):
    prompt_with_style = apply_persona(prompt)

    if not TRANSFORMERS_AVAILABLE:
        # Return a fallback response when transformers is not available
        return {
            "response": f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠØ© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Python 3.10 Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠØ©.\n\nPrompt: {prompt_with_style}",
            "model_used": "fallback",
            "persona": _current_persona
        }

    # Normal flow when transformers is available
    model_key = choose_model(context)
    tokenizer, model = load_model(model_key)

    tokens = tokenizer(prompt_with_style, return_tensors="pt")
    output = model.generate(**tokens, max_new_tokens=max_tokens, pad_token_id=tokenizer.eos_token_id)
    decoded = tokenizer.decode(output[0], skip_special_tokens=True)

    return {
        "response": decoded,
        "model_used": model_key,
        "persona": _current_persona
    }
